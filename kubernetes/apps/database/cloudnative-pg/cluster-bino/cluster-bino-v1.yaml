---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/postgresql.cnpg.io/cluster_v1.json
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: cluster-bino
spec:
  instances: 1
  imageName: ghcr.io/cloudnative-pg/postgis:16-3.4-63
  primaryUpdateStrategy: unsupervised
  storage:
    size: 64Gi
    storageClass: openebs-hostpath
  superuserSecret:
    name: cloudnative-pg-secret
  enableSuperuserAccess: true
  postgresql:
    shared_preload_libraries: ["pg_stat_statements"]
    parameters:
      statement_timeout: "0"
      max_connections: "600"
      max_slot_wal_keep_size: 10GB
      shared_buffers: 512MB
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
    limits:
      memory: 8Gi
  monitoring:
    enablePodMonitor: true
    # Ref: https://github.com/cloudnative-pg/cloudnative-pg/issues/2501
    podMonitorMetricRelabelings:
      - { sourceLabels: ["cluster"], targetLabel: cnpg_cluster, action: replace }
      - { regex: cluster, action: labeldrop }
  backup:
    barmanObjectStore: &barmanObjectStore
      data:
        jobs: 6 # Just seems like a reasonable amount, not to do with cpu.
        compression: bzip2
      destinationPath: s3://sol-cloudnative-pg/
      endpointURL: https://s3.us-east-005.backblazeb2.com
      # Note: serverName version needs to be inclemented
      # when recovering from an existing cnpg cluster
      serverName: &currentCluster cluster-bino-v1
      s3Credentials:
        accessKeyId:
          name: cloudnative-pg-secret
          key: aws-access-key-id
        secretAccessKey:
          name: cloudnative-pg-secret
          key: aws-secret-access-key
